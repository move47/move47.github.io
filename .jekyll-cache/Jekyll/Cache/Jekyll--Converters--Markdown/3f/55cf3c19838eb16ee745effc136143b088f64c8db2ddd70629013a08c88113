I"Y<h3 id="background-and-rationale">Background and Rationale</h3>
<p>Mobile phones, wearable devices, voice assistants, and autonomous vehicles are just a few of the new
distributed networks generating a wealth of data each day, where each data sample belongs to different type of statistical distribution(Non- IID’s) . Due to the growing computational power of these devices—coupled with concerns about transmitting private information(if it gets leaked, a lot about the device and the user’s behavior can be inferred easily)—it is increasingly attractive to store data locally and thus, push network computation closer to the edge devices. The learning becomes more challenging as it seems if data contains sensitive information like location, health, and other ambient signals because the private information gets more sensitive over time, which may lead to bad user experience. <strong><em>Federated learning</em></strong> <em>has emerged as a new training paradigm in such settings. Federated learning (aka collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging their data samples</em>. It stands in contrast to traditional centralized machine learning techniques where all data samples are uploaded to one server, as well as to more classical decentralized approaches which assume that local data samples are identically distributed. From <a href="#references"><span style="color:green">[1]</span></a>, FL is privacy-preserving model training in heterogeneous,distributed networks.</p>
:ET